{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import array, col, explode, lit, struct, split, mean, stddev, lead, lag, concat, count, year, month, dayofmonth\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Iterable \n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\")\\\n",
    "                            .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "                            .getOrCreate()\n",
    "\n",
    "from util import *\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTakes orderbook features (from Intermediate layer) and aggregates them to different time buckets.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Takes orderbook features (from Intermediate layer) and aggregates them to different time buckets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange = 'FTX'\n",
    "symbol = 'BTC-PERP'\n",
    "\n",
    "resample_buckets = [10, 30, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.load(f'data/02_intermediate/lob/exchange={exchange}/symbol={symbol}/year=2019/*/*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sec_bucket in resample_buckets:\n",
    "    df = df.withColumn(f'dt_resampled_{sec_bucket}s', resample(df.timestamp, agg_interval=sec_bucket))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sec_bucket in resample_buckets:\n",
    "    df_agg = df.groupby(f'dt_resampled_{sec_bucket}s') \\\n",
    "        .agg(\n",
    "            count(f'dt_resampled_{sec_bucket}s'),\n",
    "            mean('spread'),\n",
    "            stddev('spread'),\n",
    "            mean('midprice'),\n",
    "            stddev('midprice'),\n",
    "            mean('bbo_imbalance'),\n",
    "            stddev('bbo_imbalance'),\n",
    "            mean('book_imbalance'),\n",
    "            stddev('book_imbalance'),\n",
    "    ).sort(f'dt_resampled_{sec_bucket}s')\n",
    "\n",
    "    df_agg = df_agg.withColumnRenamed(f\"dt_resampled_{sec_bucket}s\", \"timestamp\") \\\n",
    "                    .withColumnRenamed(f\"count(dt_resampled_{sec_bucket}s)\", \"count_events\") \\\n",
    "                    .withColumnRenamed(\"avg(spread)\", \"spread_mean\") \\\n",
    "                    .withColumnRenamed(\"stddev_samp(spread)\", \"spread_std\") \\\n",
    "                    .withColumnRenamed(\"avg(midprice)\", \"midprice_mean\") \\\n",
    "                    .withColumnRenamed(\"stddev_samp(midprice)\", \"midprice_std\") \\\n",
    "                    .withColumnRenamed(\"avg(bbo_imbalance)\", \"bbo_imbalance_mean\") \\\n",
    "                    .withColumnRenamed(\"stddev_samp(bbo_imbalance)\", \"bbo_imbalance_std\") \\\n",
    "                    .withColumnRenamed(\"avg(book_imbalance)\", \"book_imbalance_mean\") \\\n",
    "                    .withColumnRenamed(\"stddev_samp(book_imbalance)\", \"book_imbalance_std\") \n",
    "\n",
    "    df_agg = df_agg.withColumn('exchange', lit(exchange)) \\\n",
    "                    .withColumn('symbol', lit(symbol)) \\\n",
    "                    .withColumn(\"year\", year(df_agg.timestamp)) \\\n",
    "                    .withColumn(\"month\", month(df_agg.timestamp)) \\\n",
    "                    .withColumn(\"day\", dayofmonth(df_agg.timestamp)) \\\n",
    "                    .withColumn(\"bucket_size\", lit(f\"{sec_bucket}_second_bucketing\"))\n",
    "\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "    df_agg.write.mode('overwrite').partitionBy('exchange', 'symbol', 'bucket_size', 'year', 'month', 'day').parquet(\"data/03_feature/lob\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
